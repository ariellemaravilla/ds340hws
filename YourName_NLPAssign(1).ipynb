{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing (72 pts)\n",
    "\n",
    "For this problem set, you'll also need the file conll2003train.txt if you are working in Google Colab.\n",
    "\n",
    "If you are working in the SCC:\n",
    "\n",
    "* List of modules to load: miniconda academic-ml/fall-2025\n",
    "\n",
    "* Pre-Launch Command: conda activate fall-2025-pyt \n",
    "\n",
    "* When you would load 'conll2003train.txt', instead load '/projectnb/ds340/materials/conll2003train.txt'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===> You need to **restart the notebook** after running this cell to get the right module versions\n",
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dn_2r_ChBYVN"
   },
   "source": [
    "# Part I.  Named Entity Recognition (25 pts)\n",
    "\n",
    "Named entity recognition (NER) is a classic NLP task where proper nouns and their types must be extracted from text.  The CONLL 2003 dataset labels entities in text as PER (person), ORG (organization), LOC (location), MISC (proper noun but none of the above), or O (nothing).  A classifier trained on this data can label each word in a sentence as belonging to one of these categories.\n",
    "\n",
    "In this section, we'll use word2vec vectors to classify each word.  Word2vec doesn't use any context from the rest of the sentence, but the task of identifying proper nouns as places or people may not need a lot of context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Jrz5S5P2oRGo"
   },
   "outputs": [],
   "source": [
    "# CONLL (Computational Natural Language Learning) 2003\n",
    "# data from:\n",
    "# https://data.deepai.org/conll2003.zip\n",
    "# description of data:\n",
    "# https://huggingface.co/datasets/eriktks/conll2003\n",
    "\n",
    "from google.colab import files\n",
    "# Pick conll2003train.txt for full training\n",
    "uploaded = files.upload()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8J59HKYWvdp8"
   },
   "outputs": [],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bnXNpV7Ty-RQ"
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "# Load the data as list of tuples of lists:\n",
    "# in each tuple, the zeroth element is the list of words\n",
    "# and the first element is the list of labels.\n",
    "#\n",
    "# Original data has line breaks to separate sentences and\n",
    "# -DOCSTART- at the beginning of lines separating documents;\n",
    "# on each line, item 0 is the word and item 4 is the NER label.\n",
    "# We don't care about the distinction between B- and I-\n",
    "# (begin and intra) for the NER labels, so we just keep\n",
    "# the category and turn it to a number\n",
    "# (0 = nothing, 1 = PER, 2 = ORG, 3 = LOC, 4 = MISC)\n",
    "def load_ner_data(filename):\n",
    "  lines = []\n",
    "  with open(filename, mode='r') as myfile:\n",
    "    spacereader = csv.reader(myfile, delimiter=' ')\n",
    "    working_sentence = []\n",
    "    working_ner_tags = []\n",
    "    for row in spacereader:\n",
    "      if len(row) == 0:\n",
    "        if len(working_sentence) > 0:\n",
    "          lines.append((working_sentence, working_ner_tags))\n",
    "          working_sentence = []\n",
    "          working_ner_tags = []\n",
    "      elif len(row) == 4:\n",
    "        if row[0] != '-DOCSTART-':\n",
    "          working_sentence.append(row[0])\n",
    "          working_ner_tags.append(process_ner_tag(row[3]))\n",
    "  return lines\n",
    "\n",
    "def process_ner_tag(tag):\n",
    "  if tag == 'O': # Not 0, for some odd reason...\n",
    "    return 0\n",
    "  tag = tag[2:] # Ignore B-, I-\n",
    "  tag_dict = {\n",
    "      'PER': 1,\n",
    "      'ORG': 2,\n",
    "      'LOC': 3,\n",
    "      'MISC': 4\n",
    "  }\n",
    "  # Intentionally error if we get none of the above\n",
    "  return tag_dict[tag]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0ifvwsmRDPS9"
   },
   "source": [
    "The following line should load the relevant data as a list of tuples, where the first element of each tuple is a list of words in a sentence, and the second element of each tuple is a list of the words' proper numerical labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RuiirmeH1zzU"
   },
   "outputs": [],
   "source": [
    "all_tuples = load_ner_data('conll2003train.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tlafI5VstE25"
   },
   "outputs": [],
   "source": [
    "len(all_tuples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R6V7cBwaVIYi"
   },
   "outputs": [],
   "source": [
    "print(all_tuples[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yM4kP3-TE2eG"
   },
   "source": [
    "For faster processing, we'll just work with the first 1000 sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GI0UjuGttKuf"
   },
   "outputs": [],
   "source": [
    "MAX_SENTENCES = 1000\n",
    "tuples = all_tuples[:MAX_SENTENCES]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dsYpFAlzEs2R"
   },
   "source": [
    "I.1, 10 points) Write a function words_to_word2vec() that converts the conll2003 data into a feature matrix and label array of the kind expected by scikit-learn.\n",
    "\n",
    "The first argument to the function should be a list of tuples of the kind produced by load_ner_data().\n",
    "\n",
    "The second argument to the function should be a model of the kind returned by gensim.downloader.load().  A call creating one of these objects has been provided.\n",
    "\n",
    "The first return value should be a $W \\times 300$ feature matrix, where $W$ is the number of words in all the input sentences combined, and 300 is the number of elements in each vector returned by the word vector model.  If a word can't be found in the word model, the corresponding line should be all zeros.\n",
    "\n",
    "The second return value should be a 1d $W$-element array of the labels of the words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FCzIQ8RfCITz"
   },
   "outputs": [],
   "source": [
    "# Approach #1:  Convert every vector using word2vec,\n",
    "# and train a scikit-learn classifier on these vectors.\n",
    "\n",
    "import gensim.downloader as api\n",
    "\n",
    "wv = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ks9DHwNaC9ep"
   },
   "outputs": [],
   "source": [
    "# TODO words_to_word2vec_matrix(tuple_list, wv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xyxxYXs8En40"
   },
   "outputs": [],
   "source": [
    "# Test of words_to_word2vec_matrix\n",
    "features, labels = words_to_word2vec_matrix([(['Sonic', 'is', 'fast'], [1, 0, 0])], wv)\n",
    "print(features.shape) # expect (3, 300)\n",
    "print(labels.shape) # expect (3,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RFMzksqwHg2z"
   },
   "source": [
    "I.2, 5 points) Now, perform a train/test split on your feature matrix and labels (with test_size = 0.1 and random_state=340) and measure the accuracy of a RandomForestClassifier with 200 estimators (and also random_state=340, other settings default) that uses your word2vec matrix as its features.  You can expect roughly 94% accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mP48ByCKE8sg"
   },
   "outputs": [],
   "source": [
    "# TODO Train a random forest on the matrix\n",
    "# Use random seed 340 for train_test_split() and random forest;\n",
    "# test size 10% for train_test_split; 200 trees for forest\n",
    "# and otherwise defaults"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fd7FhYd_ICpF"
   },
   "source": [
    "I.3, 4 pts) This is a task where \"not a proper noun\" is a common category and a pretty good guess, inflating accuracy.  Call sklearn.metrics.precision_recall_fscore_support to see precision, recall, and f-scores for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8bNXd_b2Kff9"
   },
   "outputs": [],
   "source": [
    "# TODO precision, recall, f-scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y1uqloMUJUSB"
   },
   "source": [
    "I.4, 6 pts)  Identify which class had the lowest *precision* and what it means to have that precision.  Then do the same for *recall*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "emXdm0pPJxPY"
   },
   "source": [
    "**TODO precision**\n",
    "\n",
    "**TODO recall**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IIv9iNkFIuUk"
   },
   "source": [
    "# Part II.  Attention (15 pts)\n",
    "\n",
    "We are now going to walk through an example of how attention could be computed in a sentence.  This omits the multiplication by learned matrices, but captures how the main mechanism of attention alters word embeddings.\n",
    "\n",
    "II.1, 2 pts)  Use the word vector model that you used in the last problem to look up a list of vectors for the following two sentences:\n",
    "\n",
    "* \"Turkey closed its borders today.\"\n",
    "* \"Turkey is a Thanksgiving tradition.\"\n",
    "\n",
    "As before, if a word isn't in the model, use a 300 element zero vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k5xYmnsMMcc8"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "sentence1 = ['Turkey', 'closed', 'its', 'borders', 'today']\n",
    "sentence2 = ['Turkey', 'is', 'a', 'Thanksgiving', 'tradition']\n",
    "\n",
    "# TODO transform to lists of vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6vHdHqpONoUU"
   },
   "source": [
    "II.2, 3 pts) For just the word \"Turkey\", find the dot product of its vector with each other vector in each sentence.  Report which word has the largest dot product in each sentence (besides the word itself)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CJm7mXrmNnFM"
   },
   "outputs": [],
   "source": [
    "# TODO sentence 1 dots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IlgoHb-qOdRS"
   },
   "source": [
    "**TODO note word with largest dot**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A9i5qCG-OT7e"
   },
   "outputs": [],
   "source": [
    "# TODO sentence 2 dots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L6KNXm2yOxbd"
   },
   "source": [
    "**TODO note word with largest dot**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b1Dk4m1qP5kZ"
   },
   "source": [
    "II.3, 3 pts) Use the softmax formula, $\\frac{e^{x_i}}{\\sum_i e^{x_i}}$, on each element of each of the dot product lists to create a list of weights that sum to 1 in each case.  Don't include the \"Turkey\" vector dot product in the calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zG83sOTVM1bY"
   },
   "outputs": [],
   "source": [
    "# TODO softmax result 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7d4bqUFXYNvB"
   },
   "outputs": [],
   "source": [
    "# TODO softmax result 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WyCeN3jjYVTV"
   },
   "source": [
    "II.4, 3 pts) Create a single vector for each sentence that is Turkey's attention vector:  the weighted sum of the four vectors that don't correspond to the word \"Turkey\", where the weights were created by the softmax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fQOcnVz3YUSt"
   },
   "outputs": [],
   "source": [
    "# TODO attention weights 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JQn3qlicY0AJ"
   },
   "outputs": [],
   "source": [
    "# TODO attention weights 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MBg1IN5nZPB0"
   },
   "source": [
    "II.5, 4 pts) Run the classifier for part 1 on 3 vectors:  the plain \"Turkey\" vector, the \"Turkey\" vector with WEIGHT times the sentence 1 attention vector added, and the \"Turkey\" vector with WEIGHT times sentence 2 attention vector added.  Experiment with values of WEIGHT until you find a setting where the first sentence's attention-modified Turkey vector is a location, but the second is not.  (The learned value matrices in actual attention can accomplish what WEIGHT is doing here, and more.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z2WJDNp7ZGQe"
   },
   "outputs": [],
   "source": [
    "# TODO code that finds weight that causes classifier to label one Turkey a location, the other not"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qAp6salmeBHs"
   },
   "source": [
    "# Part III.  Using a pretrained language model \"off-the-shelf\" (24 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5eLwvI48j0OK"
   },
   "source": [
    "Now, we'll try using a model that is a step up from word vectors - a BERT model that has been trained to produce a vector for each word in the sentence that is informed by attention.  We'll also change tasks, since the Google-News-trained word2vec seemed pretty good already for the CONLL2003 NER task.\n",
    "\n",
    "The JNLPBA dataset is like CONLL 2003, but labels words as to whether they are words for DNA, RNA, proteins, cell lines, or cell types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2krS61Vmzmvy"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Dataset description: https://huggingface.co/datasets/jnlpba/jnlpba\n",
    "\n",
    "NLP for labeling biological terms.  Original labels (thanks to\n",
    "https://medium.com/@raj.pulapakura/fine-tune-your-own-bert-token-classification-model-06b1153fbf56):\n",
    "\n",
    "    0: O => ordinary word\n",
    "    1: B-DNA => beginning of a “DNA” term\n",
    "    2: I-DNA => continuation of a “DNA” term\n",
    "    3: B-RNA=> beginning of an “RNA” term\n",
    "    4: I-RNA => contiunation of an “RNA” term\n",
    "    5: B-protein => beginning of a “protein” term\n",
    "    6: I-protein => continuation of a “protein” term\n",
    "    7: B-cell_line => beginning of a “cell line” term\n",
    "    8: I-cell_line => continuation of a “cell line” term\n",
    "    9: B-cell_type => beginning of a “cell type” term\n",
    "    10: I-cell_type => continuation of a “cell type” term\n",
    "\n",
    "    We will lump B- and I- labels together - it'll be an easier task if\n",
    "    the classifier doesn't have to figure out word position in the sentence.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VakcRiC80I-9"
   },
   "outputs": [],
   "source": [
    "!pip install transformers datasets evaluate seqeval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AXuDwN9P0Vtj"
   },
   "outputs": [],
   "source": [
    "# This time, we'll also make use of Huggingface Datasets.\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "raw_dataset = load_dataset(\"siddharthtumre/jnlpba-split\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HYqdDt6I0o7a"
   },
   "source": [
    "Let's take a look at what a HuggingFace dataset looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HIQuuPyK0lMg"
   },
   "outputs": [],
   "source": [
    "raw_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kYf9N0lx0uok"
   },
   "outputs": [],
   "source": [
    "raw_dataset['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vUeVIq_B0xvK"
   },
   "outputs": [],
   "source": [
    "# Some BERT code adapted from\n",
    "# https://github.com/jalammar/jalammar.github.io/blob/master/notebooks/bert/A_Visual_Notebook_to_Using_BERT_for_the_First_Time.ipynb\n",
    "\n",
    "import transformers as ppb\n",
    "\n",
    "WEIGHTS = 'distilbert-base-uncased'\n",
    "def get_tokenizer():\n",
    "    return ppb.DistilBertTokenizer.from_pretrained(WEIGHTS)\n",
    "\n",
    "tokenizer = get_tokenizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "96L2yc6F1AjJ"
   },
   "source": [
    "Note that the BERT tokenizer may break a word into parts if it doesn't recognize the whole word.  \"Ohioization\" becomes two tokens, \"Ohio\" and \"##ization.\"\n",
    "\n",
    "The tokenizer thinks of everything as a sentence and concatenates begin and end tokens to its tokenizations.  The following function strips these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l-bdEztJ09K8"
   },
   "outputs": [],
   "source": [
    "# Convert a word into BERT tokens\n",
    "def get_tokens(word, tokenizer):\n",
    "    token_list = tokenizer.encode(word)\n",
    "    return token_list[1:-1] # Strip begin and end tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PAOZGD7y1NXE"
   },
   "source": [
    "III.1, 9 points) Write a function dataset_to_bert_input_and_labels that takes one of the Huggingface datasets (the 'train' set), the tokenizer, and a maximum number of sentences, and returns a 2D array with as many sentences as there were in the data (or max_sentences, whichever is smaller) and as many columns as are necessary for the longest number of tokens, plus two.  Each token list should start with 101 (the CLS token) and end with 102 (end) - hence the +2.\n",
    "\n",
    "In addition, force all the B- labels (odd numbers) to be the corresponding I- labels (even numbers one larger).\n",
    "\n",
    "You can construct your first output as a list-of-lists at first, and in a second pass through the sentences, pad each of your lists with 0's, so that they are all the same length.  Then convert to 2D array.\n",
    "\n",
    "So, for example, `dataset_to_bert_input_and_labels(raw_dataset['train'], tokenizer, 2)` should produce two return values - the first, a 2D array that looks like np.array([[101, ..., 0], [101, ..., 102]]), and the second, a list of label lists where the two lists are composed of 8's, 10's, and 0's."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OwtuFc_n2HWm"
   },
   "outputs": [],
   "source": [
    "# Turn the lists of words into padded arrays of token numbers\n",
    "# TODO dataset_to_bert_input_and_labels(dataset, tokenizer, max_sentences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YZHnxD5M20in"
   },
   "outputs": [],
   "source": [
    "dataset_to_bert_input_and_labels(raw_dataset['train'], tokenizer, 2) # See instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1IlhNyhx5gQ1"
   },
   "source": [
    "The following code should then produce a $800 \\times 180 \\times 768$ tensor - 800 sentences with at most 180 tokens, each of which has a 768 element vector associated with it.  This may take a little while as each token list is run through the pretrained BERT network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9atzMq8j5YOs"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Grab a trained DistiliBERT model\n",
    "def get_model():\n",
    "    return ppb.DistilBertModel.from_pretrained(WEIGHTS)\n",
    "\n",
    "def get_bert_vectors(model, padded_tokens):\n",
    "    # Mask the 0's padding from attention\n",
    "    mask = torch.tensor(np.where(padded_tokens != 0, 1, 0))\n",
    "    with torch.no_grad():\n",
    "        word_vecs = model(torch.tensor(padded_tokens).to(torch.int64), attention_mask=mask)\n",
    "    # The middle index of the return value determines which word we're talking about\n",
    "    # (starting with 1 since 0 is the CLS token)\n",
    "    return word_vecs[0][:,:,:].numpy()\n",
    "\n",
    "train_input, labels = dataset_to_bert_input_and_labels(raw_dataset['train'], tokenizer, 800)\n",
    "model = get_model()\n",
    "bert_result = get_bert_vectors(model, train_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BaDqHHbQ6myN"
   },
   "outputs": [],
   "source": [
    "print(bert_result.shape) # Expect (800, 180, 768)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iXKkZPfoCubM"
   },
   "source": [
    "III.2, 3 points) In your own words, what is the use of the vector associated with the 0th token, the CLS token?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dbimQ-HcC2eS"
   },
   "source": [
    "**TODO**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L5UHjubu6tgO"
   },
   "source": [
    "III.3, 8 points) Now you'll write the glue that connects the BERT part of the pipeline to some off-the-shelf ML.  Write `labels_and_bert_to_sklearn(labels, bert_result)` which takes the label list-of-lists you produced earlier and the tensor that was the result of the get_bert_vectors() call, and produce a single $W \\times 768$ features matrix and a $W$-element labels array, such that both could be used as features and labels in scikit-learn.  (By $W$, we mean the total number of words in the data.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "quGlFfQf6sH3"
   },
   "outputs": [],
   "source": [
    "# Take a list of label lists from tuples_to_bert_input_and_labels()\n",
    "# and a bert result, and create the scikit-learn features and label list\n",
    "# TODO labels_and_bert_to_sklearn(labels, bert_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4uh0V1pE37Nk"
   },
   "outputs": [],
   "source": [
    "bert_features_train, bert_labels_train = labels_and_bert_to_sklearn(labels, bert_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q8bDYbp78PuT"
   },
   "source": [
    "III.4, 4 points) Call the following code block to get test data as well.  Then train a scikit-learn RandomForestClassifier with 200 estimators and random state 340 - this will take about 6 minutes on Colab - and evaluate the classifier on the test set.  You can expect an accuracy of about 80%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pUbw7vXg8bUV"
   },
   "outputs": [],
   "source": [
    "test_input, test_labels = dataset_to_bert_input_and_labels(raw_dataset['validation'], tokenizer, 100)\n",
    "bert_result_test = get_bert_vectors(model, test_input)\n",
    "bert_features_test, bert_labels_test = labels_and_bert_to_sklearn(test_labels, bert_result_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aUXyZMf48wok"
   },
   "outputs": [],
   "source": [
    "# TODO scikit-learn on the bert vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2y9q4v03A75r"
   },
   "source": [
    "# IV.  Off-the-shelf fine-tuned model (6 points)\n",
    "\n",
    "If you want to fine-tune a BERT model, you can follow a web tutorial [here](https://learnopencv.com/fine-tuning-bert/#aioseo-fine-tuning-bert-on-the-arxiv-abstract-classification-dataset), but this takes a while.  Let's just see how much better we can do if we have a fine-tuned BERT model, versus the word2vec approach we started with.  For common datasets like CONLL2003, it's possible to find models others have already trained on the HuggingFace website.\n",
    "\n",
    "1, 6 pts) Make a prediction to yourself about what kind of F1 scores you might expect to see from this large fine-tuned transformer model.  Then run the two code boxes below to load a fine-tuned CONLL2003 NER model from HuggingFace (\"dbmdz/bert-large-cased-finetuned-conll03-english\")and evaluate it on the CONLL2003 test data.  For each class (besides O = ordinary), compare the f1 score to the f1 score for the same class in the word2vec classifier of part I.\n",
    "\n",
    "* Roughly how much of a bump in f1 score do we see for each classification, and on average?\n",
    "\n",
    "* Is the model's final performance better or worse than you expected, and why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ymZv7q_5BjtM"
   },
   "outputs": [],
   "source": [
    "# More on fine-tuning token-classification models at https://medium.com/@raj.pulapakura/fine-tune-your-own-bert-token-classification-model-06b1153fbf56\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "token_classifier = pipeline(\n",
    "  \"token-classification\",\n",
    "  \"dbmdz/bert-large-cased-finetuned-conll03-english\",\n",
    "  grouped_entities=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6AxcmN4axJiI"
   },
   "outputs": [],
   "source": [
    "# https://huggingface.co/docs/evaluate/en/base_evaluator\n",
    "\n",
    "from datasets import load_dataset\n",
    "from evaluate import evaluator\n",
    "from transformers import AutoModelForSequenceClassification, pipeline\n",
    "\n",
    "data = load_dataset(\"eriktks/conll2003\", split=\"test\", revision=\"refs/convert/parquet\").shuffle(seed=340).select(range(1000))\n",
    "task_evaluator = evaluator(\"token-classification\")\n",
    "\n",
    "eval_results = task_evaluator.compute(\n",
    "    model_or_pipeline=\"dbmdz/bert-large-cased-finetuned-conll03-english\",\n",
    "    data=data,\n",
    "    metric=\"seqeval\"\n",
    ")\n",
    "\n",
    "print(eval_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ah-eDFf6xPCj"
   },
   "source": [
    "* **TODO F1 differences**\n",
    "* **TODO compare to your predictions**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI Statement (2 pts)\n",
    "\n",
    "Please briefly describe whether and how you used generative AI for this assignment.  You will not be penalized for your answer - this is mostly so the course can adapt to AI use.\n",
    "\n",
    "**TODO**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
